{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c053d867-4580-43b2-9b6c-5f466a600211",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4f/Twitter-logo.svg\" alt=\"twitter_logo\" width=\"120\"/>\n",
    "<span style=\"float:left\">\n",
    "  <span style=\"font-family:Helvetica; font-size:4em;\">\n",
    "    <b>Twitter sentiment analysis DEV &nbsp;&nbsp;&nbsp;</b><br>\n",
    "  </span>\n",
    "  <span style=\"font-family:Helvetica; font-size:2em;\">\n",
    "    XGBoost\n",
    "  </span>\n",
    "</span>\n",
    "<br clear=\"left\"/>\n",
    "<br><br><br>\n",
    "<i>Goal of this project is to develop and ML pipeline for Twitter sentiment analysis with the end goal of hosting the ML pipeline in and ML Ops Pipeline using streaming Twitter data through the AWS architecture.</i>\n",
    "<br><br>\n",
    "  In this notebook I set up a pipeline to featurize my tweet data and train and assess multiple XGBoost models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "398c0617-d5f4-4a87-8fc8-f953b85d24ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Table of contents\n",
    "#### 1. Set-up\n",
    "* 1.1 Environment\n",
    "  * 1.1.1 Install pacages\n",
    "  * 1.1.2 Import packages\n",
    "* 1.2 User defined functions\n",
    "* 1.3 Global variables\n",
    "* 1.4 Import data\n",
    "\n",
    "#### 2. Pre-processing\n",
    "* 2.1 Transform label column\n",
    "* 2.2 Train-test split\n",
    "* 2.3 Pre-processing pipeline\n",
    "  * 2.3.1 SparkKLP pipeline\n",
    "  * 2.3.2 Vectorizer pipeline\n",
    "  * 2.3.3 Combine pipeline\n",
    "* 2.4 Trainsform train and and test features\n",
    "\n",
    "#### 3. XGBoost\n",
    "* 3.1 Baseline model\n",
    "  * 3.1.1 Build baseline model\n",
    "  * 3.1.2 Fit baseline model\n",
    "  * 3.1.3 Predict\n",
    "  * 3.2 Evaluate\n",
    "* 3.2 Hyperparam tuning\n",
    "  * 3.2.1 Set up search space and CV strategy\n",
    "* 3.3 Run the training with MLFlow\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb24e5d5-e3a8-4580-b030-ab28b4efb0b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bc1d8270-733f-4e58-b9e6-a6c57ec5d057",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "85adc170-e3ff-48cd-85be-eb30d0282891",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1.1.1 Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e19b34d4-9ec0-4a65-894e-ad1206aa689d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b>In Databricks</b><br>\n",
    "Packages installed via the compute interface<br>\n",
    "Copy/paste this in the Configuration>Spark tab when first setting up the cluster:<br>\n",
    "<br>\n",
    "spark.serializer org.apache.spark.serializer.KryoSerializer<br>\n",
    "spark.databricks.delta.preview.enabled true<br>\n",
    "spark.kryoserializer.buffer.max 2000M<br>\n",
    "<br>\n",
    "Then, navgiate to the libraries tab: <br>\n",
    "Install new > Pypi: copy paste this under package: spark-nlp==3.4.4<br>\n",
    "Install new > Maven: copy paste: com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49aab080-9de8-429f-817d-1b04aabb2e4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1.1.2 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a0ae8f3d-6410-4fb9-b693-d405b45fc93e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data wrangling\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# SparkNLP pre-processing\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.base import Finisher\n",
    "from sparknlp.annotator import Tokenizer\n",
    "from sparknlp.annotator import Normalizer\n",
    "from sparknlp.annotator import LemmatizerModel\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "from sparknlp.annotator import NGramGenerator\n",
    "from sparknlp.annotator import PerceptronModel\n",
    "\n",
    "\n",
    "# nltk for stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Pre-processing and validation\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, HashingTF, IDF, StringIndexer, CountVectorizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "# mlflow for tuning and logging\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# XGBoost\n",
    "from sparkdl.xgboost import XgboostClassifier\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f01c384-7b9b-4874-bef6-ced3f1f23b22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.2 UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "facaf67f-cf96-43ac-9d4d-4fc0478a03b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# No UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ccaec317-6988-448d-aa33-006297c41bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6291f84-e5e9-49bf-a540-ff02904b5470",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE directory for data import\n",
    "MOUNTED_DATA_PATH = '/tmp/delta/twitter_stream'\n",
    "\n",
    "# INITIALIZE spark nlp instance\n",
    "spark = sparknlp.start(spark32=True)\n",
    "\n",
    "# TROUBLESHOOTING environment\n",
    "# os.environ['PYSPARK_PIN_THREAD'] = 'true'\n",
    "# spark.conf.set('spark.databricks.clusterUsageTags.clusterPinned', 'false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23e85660-fab6-43fa-844e-4e565818f955",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.4 Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "915e4802-d7ef-4f88-8599-b112e56a85b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE list: columns to drop\n",
    "droppers = ['ts', 'tweet_id', 'author_id', 'text', 'country_code', 'location', 'score']\n",
    "\n",
    "# INTIALIZE spark dataframe & DROP columns\n",
    "sdf = ((spark.read\n",
    "             .format('delta')\n",
    "             .load(MOUNTED_DATA_PATH))\n",
    "                 .drop(*droppers)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ad1328e-f55b-405c-a417-e6a4b08f159b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29792da9-eb39-463f-805c-dc46fe3a78ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Transform label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d26f2911-b642-4922-b302-6151a8dc7490",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# REPLACE sentiment labels: Negative->0, Neutral->1, Positive->2\n",
    "sdf = sdf.withColumn('label', F.when(F.col('sentiment') == 'Positive', 2)\n",
    "                                .otherwise(F.when(F.col('sentiment') == 'Neutral', 1)\n",
    "                                            .otherwise(0))) \\\n",
    "                                .drop('sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f21b292-4dc0-4307-9b2d-e4cd03973d2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "39c9d499-2cc2-46d0-bf5d-1f46fbf73fb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE train & test datasets with 80/20 split\n",
    "sdf_train, sdf_test = sdf.randomSplit([0.8, 0.2], seed=42069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3bb0ba03-34b7-437e-9aa9-b872e6444634",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Pre-processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ec24105-1cd4-41c5-aa0a-7800ceeda7e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.3.1 SparkNLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63cd60d9-fcfb-42eb-b8c2-24863be66c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">lemma_antbnc download started this may take some time.\n",
       "Approximate size to download 907.6 KB\n",
       "\r",
       "[ | ]\r",
       "[ / ]\r",
       "[ — ]\r",
       "[ \\ ]\r",
       "[ | ]\r",
       "[ / ]\r",
       "[ — ]\r",
       "[ \\ ]\r",
       "[ | ]\r",
       "[ / ]\r",
       "[ — ]\r",
       "[ \\ ]\r",
       "[OK!]\n",
       "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
       "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[OK!]\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CONVERT tweet text to spark NLP format\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('translated_text') \\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "\n",
    "# TOKENIZE tweet text\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('tokenized')\n",
    "\n",
    "# CLEAN the data with normalizer\n",
    "# CREATE patterns to remove\n",
    "patterns = ['http', '@\\S+', '#', '[^a-zA-Z]', '\\s+']\n",
    "\n",
    "# CLEAN\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True) \\\n",
    "     .setCleanupPatterns(patterns)\n",
    "\n",
    "# LEMMATIZE cleaned tokens\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemmatized')\n",
    "\n",
    "# REMOVE stopwords\n",
    "# IMPORT stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# CREATE list of stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "# REMOVE stopwords\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(eng_stopwords)\n",
    "\n",
    "# CREATE ngrams (1-, 2-, & 3- grams) from lemmatized tokens\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['unigrams']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')\n",
    "\n",
    "# CONVERT back to string\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['ngrams'])\n",
    "\n",
    "\n",
    "\n",
    "# CREATE pipeline\n",
    "sparknlp_pipeline = Pipeline().setStages([documentAssembler, tokenizer, normalizer, lemmatizer, stopwords_cleaner, ngrammer, finisher])\n",
    "\n",
    "# Fit to train data\n",
    "# fitted_sparknlp_pipeline = sparknlp_pipeline.fit(sdf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "631c7deb-cee7-40e0-a841-58a435b1bafb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.3.2 Vectorizer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23898b06-7c8e-4e12-93b3-34afe30bb2d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE column names\n",
    "string_cols = ['lang', 'tag']\n",
    "string_cols_map = [(col, col+'_ix') for col in string_cols]\n",
    "predictors = [col[1] for col in string_cols_map] + ['ngram_idf']\n",
    "\n",
    "\n",
    "# VECTORIZE\n",
    "cv = CountVectorizer(inputCol='finished_ngrams', outputCol='ngram_cv')\n",
    "\n",
    "# INVERSE DENSE FREQUENCY\n",
    "idf = IDF(inputCol='ngram_cv', outputCol='ngram_idf', minDocFreq=8) # minDocFreq: remove sparse terms\n",
    "\n",
    "# INDEX non-text string cols\n",
    "si = [StringIndexer(inputCol = col[0], outputCol = col[1]) for col in string_cols_map]\n",
    "\n",
    "# COMBINE language index, tag index, and idf-transformed word vectors\n",
    "va = VectorAssembler(inputCols = [*predictors] , outputCol='features')\n",
    "\n",
    "# CREATE pipeline\n",
    "featurizer_pipeline = Pipeline(stages = [cv, idf, *si, va])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9308bfb8-97a0-475b-8c2e-8cb2cbd68955",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.3.3 Combine pipelines, fit, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fda421ac-9382-4cad-b6dc-e0b8c8657ceb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE pipeline of pipelines: sparnlp pipeline and featurizer pipelines\n",
    "pipeline = Pipeline(stages = [sparknlp_pipeline, featurizer_pipeline])\n",
    "\n",
    "# FIT pipeline to train data\n",
    "fitted_pipeline = pipeline.fit(sdf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be094acd-7ac2-4276-821c-cfcafd0cbd01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Transform train and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49b66cab-ac82-41e6-ba9c-258ed8f8699d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[23]: 124974</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[23]: 124974</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRANSFORM train data with pipeline\n",
    "sdf_train_prepared = fitted_pipeline.transform(sdf_train)\n",
    "\n",
    "# CACHE\n",
    "sdf_train_prepared.cache()\n",
    "sdf_train_prepared.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "82248749-f5c7-475a-a4e0-b3fd70c76e6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[24]: 31172</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[24]: 31172</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRANSFORM test data with pipeline\n",
    "sdf_test_prepared = fitted_pipeline.transform(sdf_test)\n",
    "\n",
    "# CACHE \n",
    "sdf_test_prepared.cache()\n",
    "sdf_test_prepared.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe917481-a643-4ec4-9575-7fb7080c6309",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "17f3d1b1-bc08-4dc8-9a56-36679f27fe14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.1 Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ee3f4e4f-0ba5-4ed0-bd6f-60e01e183a2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3.1.1 Build baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d4aa0e3d-c8db-4a6e-961e-2f89ddb7260f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # SETUP baseline parameters\n",
    "# xgbParams = dict(\n",
    "#     learning_rate = 0.1,\n",
    "#     max_depth = 2,\n",
    "#     missing=0.0,\n",
    "#     n_estimators = 5,\n",
    "#     objective=\"multi:softmax\",\n",
    "#     num_workers=8,               # Set this to less than or equal to number of workers in cluester for distributed training\n",
    "#     featuresCol='features',\n",
    "#     labelCol='label',\n",
    "#     nthread = 36\n",
    "#         )\n",
    "\n",
    "# # # INITIALIZE XGBoost class object with baseline parameters\n",
    "# xgb = XgboostClassifier(**xgbParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5a591f8-b753-4729-a474-e5a10e1c76e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3.1.2 Fit baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "70efc699-4967-4a11-866b-0c1cf2621690",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FIT baseline XGBoost to train\n",
    "# xgb_baseline = xgb.fit(sdf_train_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63944b9a-e307-43e3-8a4f-7bc97cd3e81b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3.1.3 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fa99e9db-f077-42da-a94f-f12c9fa880da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRANSFORM: predict test labels\n",
    "# predictions = xgb_baseline.transform(sdf_test_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fcf3cc64-840d-4a49-98ed-fefa1e847b42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3.1.4 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "048ac866-34e6-4700-8fc6-350e63e31677",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EVALUATE predictions: f1 score\n",
    "# evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', metricName='f1')\n",
    "# evaluator.evaluate(predictions)\n",
    "\n",
    "# Output: 0.6761656533718952 \n",
    "# Output2: 0.33767296156355153 Mstill better than 0.01 with default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1552f5a0-2bc4-46f1-b7ca-13085faac1bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89390949-f4f4-4bad-9d1c-9041244790f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.2 Hypterparam tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1db37326-019c-4e81-9e4b-09c154c269ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3.2.1 Set up search space params and cross validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b417e436-5418-4756-8f93-cd420c7cc21b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# INITIALIZE parameters dictionary\n",
    "xgbParams = dict(\n",
    "    missing=0.0,\n",
    "    objective=\"multi:softmax\",\n",
    "    num_workers=1,               # Set this to 1. Parallelization is done in grid search\n",
    "    featuresCol='features',\n",
    "    labelCol='label'\n",
    ")\n",
    "\n",
    "# INITIALIZE XGBoost\n",
    "xgb = XgboostClassifier(**xgbParams)\n",
    "\n",
    "\n",
    "# CREATE XGBoost parameters to gridsearch\n",
    "learning_rate = [1.0, 0.5, 0.1]\n",
    "max_depth = [9, 12]\n",
    "min_child_weight = [1, 0.5, 0]\n",
    "subsample = [0.2, 0.4, 0.6,]\n",
    "n_estimators = [50, 75, 100]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# INITIALIZE grid object\n",
    "param_grid = (\n",
    "  ParamGridBuilder()\n",
    "    .addGrid(xgb.max_depth, max_depth)\n",
    "    .addGrid(xgb.n_estimators, n_estimators)\n",
    "    .addGrid(xgb.subsample, subsample)\n",
    "    .addGrid(xgb.min_child_weight, min_child_weight)\n",
    "    .addGrid(xgb.learning_rate, learning_rate)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "\n",
    "# CREATE evaluators\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(predictionCol='prediction', metricName='f1')\n",
    "evaluator_probability = MulticlassClassificationEvaluator(predictionCol='prediction', metricName='probability')\n",
    "evaluator_logloss = MulticlassClassificationEvaluator(predictionCol='prediction', metricName='logLoss')\n",
    "evaluator_recall = MulticlassClassificationEvaluator(predictionCol='prediction', metricName='weightedRecall')\n",
    "\n",
    "\n",
    "\n",
    "# CREATE cross validation object\n",
    "cv = CrossValidator(\n",
    "    parallelism=36,    # NOTE. Set this parameter to the number of workers in your Databricks enviornment\n",
    "    estimator=xgb,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator_f1,\n",
    "    numFolds=5,\n",
    "    seed=42069,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a5a6587d-d958-47f2-aee6-72580503b111",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sdf_train_prepared.cache()\n",
    "# sdf_train_prepared.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "32e384d0-9ae5-49b2-93f1-297271bdf2fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.3 Run the training with mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e70163ef-1a64-4cf1-80db-c087db3910fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RUN grid search with MLFlow\n",
    "with mlflow.start_run(run_name = 'XGBoost_sentiment_3'):\n",
    "    model = cv.fit(sdf_train_prepared) # This generates a bunch of models and scores them\n",
    "    \n",
    "    # GET BEST MODEL PARAMS\n",
    "    messy_param_dict = model.bestModel.extractParamMap()\n",
    "    best_params = {}\n",
    "    \n",
    "    # CREATE clean dictionary with model params\n",
    "    for param, value in messy_param_dict.items():\n",
    "        best_params[param.name] = value\n",
    "    \n",
    "    # SAVE params\n",
    "    mlflow.log_params(best_params)  # save the parameters to logs\n",
    "    \n",
    "    # SAVE best model\n",
    "    mlflow.spark.log_model(model.bestModel, 'XGBoost_best_model')\n",
    "    \n",
    "    # SAVE best model scores\n",
    "    metrics = dict(f1 = evaluator_f1.evaluate(model.bestModel.transform(sdf_test_prepared)),\n",
    "                   logloss = evaluator_logloss.evaluate(model.bestModel.transform(sdf_test_prepared)),\n",
    "                   recall = evaluator_recall.evaluate(model.bestModel.transform(sdf_test_prepared))\n",
    "                    )\n",
    "    # LOG metrics\n",
    "    mlflow.log_metrics(metrics)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "xgboost_sentiment_classifier_",
   "notebookOrigID": 115,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
